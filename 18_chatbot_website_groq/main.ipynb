{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering System\n",
    "### TOPIC: Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai import LLM\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_website(website):\n",
    "    response = requests.get(website)\n",
    "    if response.status_code == 200:\n",
    "        out = []\n",
    "        page_content = response.content\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            out.append(paragraph.text)\n",
    "        to_go =  '\\n'.join(out)\n",
    "        if len(to_go) >2000:\n",
    "            to_go = to_go[:2000]\n",
    "        return to_go\n",
    "    else: return 'Failed to retrieve data from the page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.ini') as f:\n",
    "    api = f.read()\n",
    "llm = LLM(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(qn, info):\n",
    "    prompt_template = f'''\n",
    "You are a Question Answering system that will accept retrieved data from a website. Using the retrived data from the website, answer the query. \n",
    "Remember that all the retrived data need not be used. Use only the relevant data present in the retrieved information and complete. \n",
    "If the answer to the query is not present in the website, reply that the answer is not found in the given webpage.\n",
    "\n",
    "Query: {qn}\n",
    "\n",
    "Retrived information: \n",
    "{info}\n",
    "\n",
    "Answer: '''\n",
    "    out = llm.generate(prompt_template)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBSITE: https://techcrunch.com/2024/04/18/meta-releases-llama-3-claims-its-among-the-best-open-models-available/\n",
      "Q: what is the blog about\n",
      "A: The blog is about Meta's latest release of open generative AI models, specifically the Llama 3 series, which includes two models, Llama 3 8B and Llama 3 70B, and their performance compared to other AI models.\n",
      "\n",
      "Q: how good are these models?\n",
      "A: According to the retrieved information, Meta claims that Llama 3 models are a \"major leap\" compared to previous models, and are among the best-performing generative AI models available today. The models have achieved high scores on various AI benchmarks, beating other open models in at least nine benchmarks. Additionally, the larger-parameter-count Llama 3 model, Llama 3 70B, is competitive with flagship generative AI models. It is also mentioned that users of the new Llama models can expect more \"steerability\", lower likelihood to refuse to answer questions, and higher accuracy on trivia questions. Therefore, it can be concluded that these models are good, showing significant improvements over previous models.\n",
      "\n",
      "Q: which company released llama3\n",
      "A: The company that released Llama 3 is Meta.\n",
      "\n",
      "Q: how many models are released?\n",
      "A: According to the retrieved information, Meta has released two models in its new Llama 3 family, specifically Llama 3 8B and Llama 3 70B, with more models to come in the future. Therefore, the answer is 2 (Llama 3 8B and Llama 3 70B).\n",
      "\n",
      "Thank you for interacting with me.\n"
     ]
    }
   ],
   "source": [
    "website = input('Input website: ')\n",
    "info = read_website(website)\n",
    "print(f'WEBSITE: {website}')\n",
    "while True:\n",
    "    query = input('Ask a query: ')\n",
    "    if not query: \n",
    "        print('Thank you for interacting with me.')\n",
    "        break\n",
    "    print('Q:',query)\n",
    "    print('A:',generate_output(query, info), end='\\n\\n')\n",
    "    sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
