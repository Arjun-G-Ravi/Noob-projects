{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering System\n",
    "### TOPIC: Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from ai import LLM\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_website(website):\n",
    "    response = requests.get(website)\n",
    "    if response.status_code == 200:\n",
    "        out = []\n",
    "        page_content = response.content\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            out.append(paragraph.text)\n",
    "        return out\n",
    "    else:\n",
    "        return 'Failed to retrieve data from the page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(qn, website):\n",
    "    \n",
    "\n",
    "    prompt_template = f'''\n",
    "You are a Question Answering system that will accept retrieved data from a website. Using the retrived data from the website, answer the query. \n",
    "Remember that all the retrived data need not be used. Use only the relevant data present in the retrieved information and complete. If the answer to the query is not present in the website, reply that the answer is not found in the given webpage.\n",
    "\n",
    "Query: {qn}\n",
    "\n",
    "Retrived information: \n",
    "{read_website(website)}\n",
    "\n",
    "Answer: '''\n",
    "    # print(prompt_template)\n",
    "    out = llm.generate(prompt_template)\n",
    "    # print(out)\n",
    "    print('cow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a Question Answering system that will accept retrieved data from a dataset. Using the retrived data, answer the query. \n",
      "Remember that all the retrived data need not be used. Use only the relevant data present in the retrieved information and complete.\n",
      "\n",
      "Query: what is a cow\n",
      "\n",
      "Retrived information: \n",
      "\n",
      "\n",
      "Answer: \n",
      "Fast language models are important for a variety of reasons. Here are a few:\n",
      "\n",
      "1. Interactivity: Fast language models can provide real-time feedback and responses, which is essential for applications such as chatbots and language translation tools. Slow models can be frustrating for users who have to wait for a response.\n",
      "2. Scalability: Fast language models can handle larger volumes of data and requests, making them more suitable for use in high-traffic applications. Slow models can become bottlenecks in such scenarios.\n",
      "3. Cost-effective: Fast language models are typically more cost-effective than slow models because they require fewer computational resources. This is especially important for organizations that need to process large amounts of data or run models frequently.\n",
      "4. Energy-efficient: Fast language models require less energy to run than slow models, which is important for sustainability and reducing carbon emissions.\n",
      "5. Advancing research: Fast language models enable researchers to explore new areas of study and make new discoveries. Slow models can limit the scope of research and slow down the progress of the field.\n",
      "\n",
      "In summary, fast language models are important because they enable more interactive, scalable, cost-effective, energy-efficient, and innovative solutions for natural language processing applications.\n",
      "None\n",
      "----------\n",
      "\n",
      "You are a Question Answering system that will accept retrieved data from a dataset. Using the retrived data, answer the query. \n",
      "Remember that all the retrived data need not be used. Use only the relevant data present in the retrieved information and complete.\n",
      "\n",
      "Query: q\n",
      "\n",
      "Retrived information: \n",
      "\n",
      "\n",
      "Answer: \n",
      "Fast language models are important for a number of reasons:\n",
      "\n",
      "1. **Efficiency and scalability**: Fast language models can process and generate text more quickly, making them more efficient and scalable for real-world applications. This is especially important for large-scale natural language processing tasks such as text classification, information retrieval, and machine translation.\n",
      "2. **Interactivity**: Fast language models can enable real-time, interactive applications such as conversational agents, chatbots, and virtual assistants. These applications require fast response times, which can only be achieved with fast language models.\n",
      "3. **Low-resource environments**: Fast language models are particularly important in low-resource environments where computational resources are limited. By requiring less computational power, fast language models can still deliver high-quality language processing capabilities in these environments.\n",
      "4. **Real-time decision making**: Fast language models can enable real-time decision making in mission-critical applications such as finance and healthcare. For example, fast language models can be used to process and analyze large volumes of textual data in real-time to inform investment decisions or diagnose medical conditions quickly.\n",
      "\n",
      "Overall, fast language models are an essential component of modern natural language processing systems. By enabling efficient, scalable, and real-time language processing, they can unlock a wide range of applications and use cases across industries.\n",
      "None\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "# Ideally, the similarity should be done with a better similarity index, but this works very fine in this situation\n",
    "    query = input('You: ')\n",
    "    if not query: break\n",
    "    tokens = nltk.word_tokenize(query.lower())\n",
    "    important_indices = []\n",
    "    l = -1\n",
    "    for token in tokens:\n",
    "        if token not in sw:\n",
    "            if token in vocab:\n",
    "                important_indices.extend(vocab[token])\n",
    "            l += 1\n",
    "    out = Counter(important_indices)    \n",
    "    s = ''  \n",
    "    for i,j in out.most_common(n):\n",
    "        if j>=l:\n",
    "            s += f'Q: {x[i]}  A: {y[i]}\\n'\n",
    "\n",
    "    prompt_template = f'''\n",
    "You are a Question Answering system that will accept retrieved data from a dataset. Using the retrived data, answer the query. \n",
    "Remember that all the retrived data need not be used. Use only the relevant data present in the retrieved information and complete.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrived information: \n",
    "{s}\n",
    "\n",
    "Answer: '''\n",
    "    print(prompt_template)\n",
    "    print(llm.generate(prompt_template))\n",
    "    sleep(3)\n",
    "    print('-'*10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
