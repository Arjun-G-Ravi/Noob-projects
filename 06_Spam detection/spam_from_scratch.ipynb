{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My plan\n",
    "\n",
    "- Read dataset. Convert y into 0,1\n",
    "- Create vocabulory of dataset\n",
    "- Make each X value into a one hot of vocab vector. Maybe, add counts to it too\n",
    "- Train naive bayes on it\n",
    "- Make it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/arjun/Desktop/Datasets/spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['v1','v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v1'] = df['v1'].map({'ham':0., 'spam':1.})\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6059, 6059)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "delimiters = re.compile(r'[ ,.\"\\'?!:()]')\n",
    "porter = PorterStemmer()\n",
    "ct = 0\n",
    "for i in range(len(df)):\n",
    "    # print(df.iloc[i,1])\n",
    "    words = delimiters.split(df.iloc[i,1].lower())\n",
    "    # print(words)\n",
    "    for word in words:\n",
    "        if word.isalpha() and porter.stem(word) not in vocab:\n",
    "            vocab[porter.stem(word)] = ct\n",
    "            # print(porter.stem(word), ct)\n",
    "            ct += 1\n",
    "vocab_len = len(vocab)\n",
    "vocab_len, ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': 0,\n",
       " 'until': 1,\n",
       " 'jurong': 2,\n",
       " 'point': 3,\n",
       " 'crazi': 4,\n",
       " 'avail': 5,\n",
       " 'onli': 6,\n",
       " 'in': 7,\n",
       " 'bugi': 8,\n",
       " 'n': 9,\n",
       " 'great': 10,\n",
       " 'world': 11,\n",
       " 'la': 12,\n",
       " 'e': 13,\n",
       " 'buffet': 14,\n",
       " 'cine': 15,\n",
       " 'there': 16,\n",
       " 'got': 17,\n",
       " 'amor': 18,\n",
       " 'wat': 19,\n",
       " 'ok': 20,\n",
       " 'lar': 21,\n",
       " 'joke': 22,\n",
       " 'wif': 23,\n",
       " 'u': 24,\n",
       " 'oni': 25,\n",
       " 'free': 26,\n",
       " 'entri': 27,\n",
       " 'a': 28,\n",
       " 'wkli': 29,\n",
       " 'comp': 30,\n",
       " 'to': 31,\n",
       " 'win': 32,\n",
       " 'fa': 33,\n",
       " 'cup': 34,\n",
       " 'final': 35,\n",
       " 'tkt': 36,\n",
       " 'may': 37,\n",
       " 'text': 38,\n",
       " 'receiv': 39,\n",
       " 'question': 40,\n",
       " 'std': 41,\n",
       " 'txt': 42,\n",
       " 'rate': 43,\n",
       " 's': 44,\n",
       " 'appli': 45,\n",
       " 'dun': 46,\n",
       " 'say': 47,\n",
       " 'so': 48,\n",
       " 'earli': 49,\n",
       " 'hor': 50,\n",
       " 'c': 51,\n",
       " 'alreadi': 52,\n",
       " 'then': 53,\n",
       " 'nah': 54,\n",
       " 'i': 55,\n",
       " 'don': 56,\n",
       " 't': 57,\n",
       " 'think': 58,\n",
       " 'he': 59,\n",
       " 'goe': 60,\n",
       " 'usf': 61,\n",
       " 'live': 62,\n",
       " 'around': 63,\n",
       " 'here': 64,\n",
       " 'though': 65,\n",
       " 'freemsg': 66,\n",
       " 'hey': 67,\n",
       " 'darl': 68,\n",
       " 'it': 69,\n",
       " 'been': 70,\n",
       " 'week': 71,\n",
       " 'now': 72,\n",
       " 'and': 73,\n",
       " 'no': 74,\n",
       " 'word': 75,\n",
       " 'back': 76,\n",
       " 'd': 77,\n",
       " 'like': 78,\n",
       " 'some': 79,\n",
       " 'fun': 80,\n",
       " 'you': 81,\n",
       " 'up': 82,\n",
       " 'for': 83,\n",
       " 'still': 84,\n",
       " 'tb': 85,\n",
       " 'xxx': 86,\n",
       " 'chg': 87,\n",
       " 'send': 88,\n",
       " 'rcv': 89,\n",
       " 'even': 90,\n",
       " 'my': 91,\n",
       " 'brother': 92,\n",
       " 'is': 93,\n",
       " 'not': 94,\n",
       " 'speak': 95,\n",
       " 'with': 96,\n",
       " 'me': 97,\n",
       " 'they': 98,\n",
       " 'treat': 99,\n",
       " 'aid': 100,\n",
       " 'patent': 101,\n",
       " 'as': 102,\n",
       " 'per': 103,\n",
       " 'your': 104,\n",
       " 'request': 105,\n",
       " 'mell': 106,\n",
       " 'oru': 107,\n",
       " 'minnaminungint': 108,\n",
       " 'nurungu': 109,\n",
       " 'vettam': 110,\n",
       " 'ha': 111,\n",
       " 'set': 112,\n",
       " 'callertun': 113,\n",
       " 'all': 114,\n",
       " 'caller': 115,\n",
       " 'press': 116,\n",
       " 'copi': 117,\n",
       " 'friend': 118,\n",
       " 'winner': 119,\n",
       " 'valu': 120,\n",
       " 'network': 121,\n",
       " 'custom': 122,\n",
       " 'have': 123,\n",
       " 'select': 124,\n",
       " 'receivea': 125,\n",
       " 'prize': 126,\n",
       " 'reward': 127,\n",
       " 'claim': 128,\n",
       " 'call': 129,\n",
       " 'code': 130,\n",
       " 'valid': 131,\n",
       " 'hour': 132,\n",
       " 'had': 133,\n",
       " 'mobil': 134,\n",
       " 'month': 135,\n",
       " 'or': 136,\n",
       " 'more': 137,\n",
       " 'r': 138,\n",
       " 'entitl': 139,\n",
       " 'updat': 140,\n",
       " 'the': 141,\n",
       " 'latest': 142,\n",
       " 'colour': 143,\n",
       " 'camera': 144,\n",
       " 'co': 145,\n",
       " 'on': 146,\n",
       " 'm': 147,\n",
       " 'gonna': 148,\n",
       " 'be': 149,\n",
       " 'home': 150,\n",
       " 'soon': 151,\n",
       " 'want': 152,\n",
       " 'talk': 153,\n",
       " 'about': 154,\n",
       " 'thi': 155,\n",
       " 'stuff': 156,\n",
       " 'anymor': 157,\n",
       " 'tonight': 158,\n",
       " 'k': 159,\n",
       " 've': 160,\n",
       " 'cri': 161,\n",
       " 'enough': 162,\n",
       " 'today': 163,\n",
       " 'six': 164,\n",
       " 'chanc': 165,\n",
       " 'cash': 166,\n",
       " 'from': 167,\n",
       " 'pound': 168,\n",
       " 'cost': 169,\n",
       " 'tsandc': 170,\n",
       " 'repli': 171,\n",
       " 'hl': 172,\n",
       " 'info': 173,\n",
       " 'urgent': 174,\n",
       " 'won': 175,\n",
       " 'membership': 176,\n",
       " 'our': 177,\n",
       " 'jackpot': 178,\n",
       " 'www': 179,\n",
       " 'dbuk': 180,\n",
       " 'net': 181,\n",
       " 'lccltd': 182,\n",
       " 'pobox': 183,\n",
       " 'search': 184,\n",
       " 'right': 185,\n",
       " 'thank': 186,\n",
       " 'breather': 187,\n",
       " 'promis': 188,\n",
       " 'wont': 189,\n",
       " 'take': 190,\n",
       " 'help': 191,\n",
       " 'grant': 192,\n",
       " 'will': 193,\n",
       " 'fulfil': 194,\n",
       " 'wonder': 195,\n",
       " 'bless': 196,\n",
       " 'at': 197,\n",
       " 'time': 198,\n",
       " 'date': 199,\n",
       " 'sunday': 200,\n",
       " 'xxxmobilemovieclub': 201,\n",
       " 'use': 202,\n",
       " 'credit': 203,\n",
       " 'click': 204,\n",
       " 'wap': 205,\n",
       " 'link': 206,\n",
       " 'next': 207,\n",
       " 'messag': 208,\n",
       " 'http': 209,\n",
       " 'com': 210,\n",
       " 'oh': 211,\n",
       " 'watch': 212,\n",
       " 'eh': 213,\n",
       " 'rememb': 214,\n",
       " 'how': 215,\n",
       " 'spell': 216,\n",
       " 'hi': 217,\n",
       " 'name': 218,\n",
       " 'ye': 219,\n",
       " 'did': 220,\n",
       " 'v': 221,\n",
       " 'naughti': 222,\n",
       " 'make': 223,\n",
       " 'wet': 224,\n",
       " 'fine': 225,\n",
       " 'if': 226,\n",
       " 'thatåõ': 227,\n",
       " 'way': 228,\n",
       " 'feel': 229,\n",
       " 'gota': 230,\n",
       " 'b': 231,\n",
       " 'england': 232,\n",
       " 'macedonia': 233,\n",
       " 'dont': 234,\n",
       " 'miss': 235,\n",
       " 'news': 236,\n",
       " 'ur': 237,\n",
       " 'nation': 238,\n",
       " 'team': 239,\n",
       " 'eg': 240,\n",
       " 'tri': 241,\n",
       " 'wale': 242,\n",
       " 'scotland': 243,\n",
       " 'that': 244,\n",
       " 'serious': 245,\n",
       " 'pay': 246,\n",
       " 'first': 247,\n",
       " 'when': 248,\n",
       " 'da': 249,\n",
       " 'stock': 250,\n",
       " 'comin': 251,\n",
       " 'aft': 252,\n",
       " 'finish': 253,\n",
       " 'lunch': 254,\n",
       " 'str': 255,\n",
       " 'down': 256,\n",
       " 'lor': 257,\n",
       " 'ard': 258,\n",
       " 'smth': 259,\n",
       " 'ffffffffff': 260,\n",
       " 'alright': 261,\n",
       " 'can': 262,\n",
       " 'meet': 263,\n",
       " 'sooner': 264,\n",
       " 'just': 265,\n",
       " 'forc': 266,\n",
       " 'myself': 267,\n",
       " 'eat': 268,\n",
       " 'slice': 269,\n",
       " 'realli': 270,\n",
       " 'hungri': 271,\n",
       " 'tho': 272,\n",
       " 'suck': 273,\n",
       " 'mark': 274,\n",
       " 'get': 275,\n",
       " 'worri': 276,\n",
       " 'know': 277,\n",
       " 'sick': 278,\n",
       " 'turn': 279,\n",
       " 'pizza': 280,\n",
       " 'lol': 281,\n",
       " 'alway': 282,\n",
       " 'convinc': 283,\n",
       " 'catch': 284,\n",
       " 'bu': 285,\n",
       " 'are': 286,\n",
       " 'fri': 287,\n",
       " 'an': 288,\n",
       " 'egg': 289,\n",
       " 'tea': 290,\n",
       " 'mom': 291,\n",
       " 'left': 292,\n",
       " 'over': 293,\n",
       " 'dinner': 294,\n",
       " 'do': 295,\n",
       " 'love': 296,\n",
       " 'we': 297,\n",
       " 're': 298,\n",
       " 'pack': 299,\n",
       " 'car': 300,\n",
       " 'll': 301,\n",
       " 'let': 302,\n",
       " 'room': 303,\n",
       " 'ahhh': 304,\n",
       " 'work': 305,\n",
       " 'vagu': 306,\n",
       " 'what': 307,\n",
       " 'doe': 308,\n",
       " 'wait': 309,\n",
       " 'clear': 310,\n",
       " 'were': 311,\n",
       " 'sure': 312,\n",
       " 'sarcast': 313,\n",
       " 'whi': 314,\n",
       " 'x': 315,\n",
       " 'doesn': 316,\n",
       " 'us': 317,\n",
       " 'yeah': 318,\n",
       " 'wa': 319,\n",
       " 'apologet': 320,\n",
       " 'fallen': 321,\n",
       " 'out': 322,\n",
       " 'she': 323,\n",
       " 'actin': 324,\n",
       " 'spoilt': 325,\n",
       " 'child': 326,\n",
       " 'caught': 327,\n",
       " 'till': 328,\n",
       " 'but': 329,\n",
       " 'too': 330,\n",
       " 'badli': 331,\n",
       " 'cheer': 332,\n",
       " 'tell': 333,\n",
       " 'anyth': 334,\n",
       " 'fear': 335,\n",
       " 'of': 336,\n",
       " 'faint': 337,\n",
       " 'housework': 338,\n",
       " 'quick': 339,\n",
       " 'cuppa': 340,\n",
       " 'subscript': 341,\n",
       " 'rington': 342,\n",
       " 'uk': 343,\n",
       " 'charg': 344,\n",
       " 'pleas': 345,\n",
       " 'confirm': 346,\n",
       " 'by': 347,\n",
       " 'yup': 348,\n",
       " 'look': 349,\n",
       " 'msg': 350,\n",
       " 'again': 351,\n",
       " 'xuhui': 352,\n",
       " 'learn': 353,\n",
       " 'her': 354,\n",
       " 'lesson': 355,\n",
       " 'oop': 356,\n",
       " 'roommat': 357,\n",
       " 'done': 358,\n",
       " 'see': 359,\n",
       " 'letter': 360,\n",
       " 'decid': 361,\n",
       " 'hello': 362,\n",
       " 'saturday': 363,\n",
       " 'tomo': 364,\n",
       " 'invit': 365,\n",
       " 'pl': 366,\n",
       " 'ahead': 367,\n",
       " 'watt': 368,\n",
       " 'weekend': 369,\n",
       " 'abiola': 370,\n",
       " 'forget': 371,\n",
       " 'need': 372,\n",
       " 'crave': 373,\n",
       " 'most': 374,\n",
       " 'sweet': 375,\n",
       " 'arabian': 376,\n",
       " 'steed': 377,\n",
       " 'mmmmmm': 378,\n",
       " 'yummi': 379,\n",
       " 'rodger': 380,\n",
       " 'burn': 381,\n",
       " 'sm': 382,\n",
       " 'nokia': 383,\n",
       " 'camcord': 384,\n",
       " 'deliveri': 385,\n",
       " 'tomorrow': 386,\n",
       " 'who': 387,\n",
       " 'hope': 388,\n",
       " 'man': 389,\n",
       " 'well': 390,\n",
       " 'endow': 391,\n",
       " 'am': 392,\n",
       " 'inch': 393,\n",
       " 'didn': 394,\n",
       " 'hep': 395,\n",
       " 'immunis': 396,\n",
       " 'nigeria': 397,\n",
       " 'fair': 398,\n",
       " 'tyler': 399,\n",
       " 'could': 400,\n",
       " 'mayb': 401,\n",
       " 'ask': 402,\n",
       " 'bit': 403,\n",
       " 'stubborn': 404,\n",
       " 'hospit': 405,\n",
       " 'kept': 406,\n",
       " 'weak': 407,\n",
       " 'sucker': 408,\n",
       " 'saw': 409,\n",
       " 'class': 410,\n",
       " 'gram': 411,\n",
       " 'usual': 412,\n",
       " 'run': 413,\n",
       " 'half': 414,\n",
       " 'eighth': 415,\n",
       " 'smarter': 416,\n",
       " 'almost': 417,\n",
       " 'whole': 418,\n",
       " 'second': 419,\n",
       " 'fyi': 420,\n",
       " 'ride': 421,\n",
       " 'morn': 422,\n",
       " 'crash': 423,\n",
       " 'place': 424,\n",
       " 'wow': 425,\n",
       " 'never': 426,\n",
       " 'realiz': 427,\n",
       " 'embarass': 428,\n",
       " 'accomod': 429,\n",
       " 'thought': 430,\n",
       " 'sinc': 431,\n",
       " 'best': 432,\n",
       " 'seem': 433,\n",
       " 'happi': 434,\n",
       " 'sorri': 435,\n",
       " 'give': 436,\n",
       " 'offer': 437,\n",
       " 'ac': 438,\n",
       " 'sptv': 439,\n",
       " 'new': 440,\n",
       " 'jersey': 441,\n",
       " 'devil': 442,\n",
       " 'detroit': 443,\n",
       " 'red': 444,\n",
       " 'wing': 445,\n",
       " 'play': 446,\n",
       " 'ice': 447,\n",
       " 'hockey': 448,\n",
       " 'correct': 449,\n",
       " 'incorrect': 450,\n",
       " 'end': 451,\n",
       " 'mallika': 452,\n",
       " 'sherawat': 453,\n",
       " 'yesterday': 454,\n",
       " 'find': 455,\n",
       " 'congrat': 456,\n",
       " 'year': 457,\n",
       " 'special': 458,\n",
       " 'cinema': 459,\n",
       " 'pass': 460,\n",
       " 'suprman': 461,\n",
       " 'etc': 462,\n",
       " 'later': 463,\n",
       " 'where': 464,\n",
       " 'reach': 465,\n",
       " 'gauti': 466,\n",
       " 'sehwag': 467,\n",
       " 'odi': 468,\n",
       " 'seri': 469,\n",
       " 'pick': 470,\n",
       " 'burger': 471,\n",
       " 'yourself': 472,\n",
       " 'move': 473,\n",
       " 'pain': 474,\n",
       " 'kill': 475,\n",
       " 'good': 476,\n",
       " 'girl': 477,\n",
       " 'situat': 478,\n",
       " 'seeker': 479,\n",
       " 'part': 480,\n",
       " 'check': 481,\n",
       " 'iq': 482,\n",
       " 'took': 483,\n",
       " 'forev': 484,\n",
       " 'come': 485,\n",
       " 'doubl': 486,\n",
       " 'hair': 487,\n",
       " 'dresser': 488,\n",
       " 'said': 489,\n",
       " 'wun': 490,\n",
       " 'cut': 491,\n",
       " 'short': 492,\n",
       " 'nice': 493,\n",
       " 'advis': 494,\n",
       " 'follow': 495,\n",
       " 'recent': 496,\n",
       " 'review': 497,\n",
       " 'mob': 498,\n",
       " 'award': 499,\n",
       " 'bonu': 500,\n",
       " 'dedic': 501,\n",
       " 'day': 502,\n",
       " 'which': 503,\n",
       " 'song': 504,\n",
       " 'valuabl': 505,\n",
       " 'frnd': 506,\n",
       " 'rpli': 507,\n",
       " 'complimentari': 508,\n",
       " 'trip': 509,\n",
       " 'eurodisinc': 510,\n",
       " 'trav': 511,\n",
       " 'di': 512,\n",
       " 'morefrmmob': 513,\n",
       " 'shracomorsglsuplt': 514,\n",
       " 'hear': 515,\n",
       " 'ken': 516,\n",
       " 'plane': 517,\n",
       " 'wah': 518,\n",
       " 'lucki': 519,\n",
       " 'save': 520,\n",
       " 'money': 521,\n",
       " 'hee': 522,\n",
       " 'babe': 523,\n",
       " 'im': 524,\n",
       " 'wanna': 525,\n",
       " 'someth': 526,\n",
       " 'xx': 527,\n",
       " 'perform': 528,\n",
       " 'machan': 529,\n",
       " 'onc': 530,\n",
       " 'cool': 531,\n",
       " 'gentleman': 532,\n",
       " 'digniti': 533,\n",
       " 'respect': 534,\n",
       " 'peopl': 535,\n",
       " 'veri': 536,\n",
       " 'much': 537,\n",
       " 'shi': 538,\n",
       " 'pa': 539,\n",
       " 'oper': 540,\n",
       " 'after': 541,\n",
       " 'same': 542,\n",
       " 'job': 543,\n",
       " 'ta': 544,\n",
       " 'earn': 545,\n",
       " 'ah': 546,\n",
       " 'stop': 547,\n",
       " 'urgnt': 548,\n",
       " 'real': 549,\n",
       " 'yo': 550,\n",
       " 'ticket': 551,\n",
       " 'one': 552,\n",
       " 'jacket': 553,\n",
       " 'multi': 554,\n",
       " 'start': 555,\n",
       " 'came': 556,\n",
       " 'bed': 557,\n",
       " 'coin': 558,\n",
       " 'factori': 559,\n",
       " 'gotta': 560,\n",
       " 'nitro': 561,\n",
       " 'ela': 562,\n",
       " 'kano': 563,\n",
       " 'il': 564,\n",
       " 'download': 565,\n",
       " 'wen': 566,\n",
       " 'stand': 567,\n",
       " 'close': 568,\n",
       " 'anoth': 569,\n",
       " 'night': 570,\n",
       " 'spent': 571,\n",
       " 'late': 572,\n",
       " 'afternoon': 573,\n",
       " 'casualti': 574,\n",
       " 'mean': 575,\n",
       " 'haven': 576,\n",
       " 'ani': 577,\n",
       " 'y': 578,\n",
       " 'includ': 579,\n",
       " 'sheet': 580,\n",
       " 'smile': 581,\n",
       " 'pleasur': 582,\n",
       " 'troubl': 583,\n",
       " 'pour': 584,\n",
       " 'rain': 585,\n",
       " 'hurt': 586,\n",
       " 'becoz': 587,\n",
       " 'someon': 588,\n",
       " 'servic': 589,\n",
       " 'repres': 590,\n",
       " 'between': 591,\n",
       " 'guarante': 592,\n",
       " 'havent': 593,\n",
       " 'plan': 594,\n",
       " 'buy': 595,\n",
       " 'lido': 596,\n",
       " 'show': 597,\n",
       " 'collect': 598,\n",
       " 'simpli': 599,\n",
       " 'password': 600,\n",
       " 'verifi': 601,\n",
       " 'usher': 602,\n",
       " 'britney': 603,\n",
       " 'fml': 604,\n",
       " 'telugu': 605,\n",
       " 'movi': 606,\n",
       " 'abt': 607,\n",
       " 'load': 608,\n",
       " 'loan': 609,\n",
       " 'wk': 610,\n",
       " 'hol': 611,\n",
       " 'forgot': 612,\n",
       " 'hairdress': 613,\n",
       " 'appoint': 614,\n",
       " 'four': 615,\n",
       " 'shower': 616,\n",
       " 'beforehand': 617,\n",
       " 'caus': 618,\n",
       " 'prob': 619,\n",
       " 'noth': 620,\n",
       " 'els': 621,\n",
       " 'okay': 622,\n",
       " 'price': 623,\n",
       " 'long': 624,\n",
       " 'legal': 625,\n",
       " 'them': 626,\n",
       " 'ave': 627,\n",
       " 'gone': 628,\n",
       " 'drive': 629,\n",
       " 'test': 630,\n",
       " 'yet': 631,\n",
       " 'guess': 632,\n",
       " 'gave': 633,\n",
       " 'boston': 634,\n",
       " 'men': 635,\n",
       " 'chang': 636,\n",
       " 'locat': 637,\n",
       " 'nyc': 638,\n",
       " 'cuz': 639,\n",
       " 'signin': 640,\n",
       " 'page': 641,\n",
       " 'umma': 642,\n",
       " 'life': 643,\n",
       " 'vava': 644,\n",
       " 'lot': 645,\n",
       " 'dear': 646,\n",
       " 'wish': 647,\n",
       " 'birthday': 648,\n",
       " 'truli': 649,\n",
       " 'memor': 650,\n",
       " 'aight': 651,\n",
       " 'hit': 652,\n",
       " 'would': 653,\n",
       " 'ip': 654,\n",
       " 'address': 655,\n",
       " 'consid': 656,\n",
       " 'comput': 657,\n",
       " 'isn': 658,\n",
       " 'minecraft': 659,\n",
       " 'server': 660,\n",
       " 'grumpi': 661,\n",
       " 'old': 662,\n",
       " 'better': 663,\n",
       " 'lie': 664,\n",
       " 'busi': 665,\n",
       " 'plural': 666,\n",
       " 'noun': 667,\n",
       " 'research': 668,\n",
       " 'thing': 669,\n",
       " 'scare': 670,\n",
       " 'mah': 671,\n",
       " 'loud': 672,\n",
       " 'gent': 673,\n",
       " 'contact': 674,\n",
       " 'last': 675,\n",
       " 'draw': 676,\n",
       " 'openin': 677,\n",
       " 'sentenc': 678,\n",
       " 'formal': 679,\n",
       " 'anyway': 680,\n",
       " 'juz': 681,\n",
       " 'tt': 682,\n",
       " 'eatin': 683,\n",
       " 'puttin': 684,\n",
       " 'weight': 685,\n",
       " 'haha': 686,\n",
       " 'anythin': 687,\n",
       " 'happen': 688,\n",
       " 'enter': 689,\n",
       " 'cabin': 690,\n",
       " 'boss': 691,\n",
       " 'felt': 692,\n",
       " 'askd': 693,\n",
       " 'apart': 694,\n",
       " 'went': 695,\n",
       " 'holiday': 696,\n",
       " 'flight': 697,\n",
       " 'inc': 698,\n",
       " 'goodo': 699,\n",
       " 'must': 700,\n",
       " 'friday': 701,\n",
       " 'ratio': 702,\n",
       " 'tortilla': 703,\n",
       " 'hmm': 704,\n",
       " 'uncl': 705,\n",
       " 'inform': 706,\n",
       " 'school': 707,\n",
       " 'directli': 708,\n",
       " 'food': 709,\n",
       " 'privat': 710,\n",
       " 'account': 711,\n",
       " 'statement': 712,\n",
       " 'unredeem': 713,\n",
       " 'identifi': 714,\n",
       " 'expir': 715,\n",
       " 'landlin': 716,\n",
       " 'malarki': 717,\n",
       " 'voda': 718,\n",
       " 'number': 719,\n",
       " 'match': 720,\n",
       " 'quot': 721,\n",
       " 'standard': 722,\n",
       " 'app': 723,\n",
       " 'sao': 724,\n",
       " 'mu': 725,\n",
       " 'ìï': 726,\n",
       " 'predict': 727,\n",
       " 'yetund': 728,\n",
       " 'hasn': 729,\n",
       " 'sent': 730,\n",
       " 'bother': 731,\n",
       " 'involv': 732,\n",
       " 'shouldn': 733,\n",
       " 'impos': 734,\n",
       " 'apologis': 735,\n",
       " 'del': 736,\n",
       " 'bak': 737,\n",
       " 'sum': 738,\n",
       " 'lucyxx': 739,\n",
       " 'tmorrow': 740,\n",
       " 'answer': 741,\n",
       " 'sunshin': 742,\n",
       " 'quiz': 743,\n",
       " 'q': 744,\n",
       " 'top': 745,\n",
       " 'soni': 746,\n",
       " 'dvd': 747,\n",
       " 'player': 748,\n",
       " 'countri': 749,\n",
       " 'algarv': 750,\n",
       " 'ansr': 751,\n",
       " 'sp': 752,\n",
       " 'tyron': 753,\n",
       " 'laid': 754,\n",
       " 'dog': 755,\n",
       " 'direct': 756,\n",
       " 'join': 757,\n",
       " 'largest': 758,\n",
       " 'bt': 759,\n",
       " 'txting': 760,\n",
       " 'gravel': 761,\n",
       " 'nt': 762,\n",
       " 'haf': 763,\n",
       " 'msn': 764,\n",
       " 'him': 765,\n",
       " 'befor': 766,\n",
       " 'activ': 767,\n",
       " 'chat': 768,\n",
       " 'svc': 769,\n",
       " 'hardcor': 770,\n",
       " 'age': 771,\n",
       " 'yr': 772,\n",
       " 'lazi': 773,\n",
       " 'type': 774,\n",
       " 'lect': 775,\n",
       " 'pouch': 776,\n",
       " 'sir': 777,\n",
       " 'mail': 778,\n",
       " 'swt': 779,\n",
       " 'tire': 780,\n",
       " 'littl': 781,\n",
       " 'lovabl': 782,\n",
       " 'person': 783,\n",
       " 'coz': 784,\n",
       " 'somtim': 785,\n",
       " 'those': 786,\n",
       " 'occupi': 787,\n",
       " 'biggest': 788,\n",
       " 'their': 789,\n",
       " 'heart': 790,\n",
       " 'gud': 791,\n",
       " 'open': 792,\n",
       " 'ya': 793,\n",
       " 'dot': 794,\n",
       " 'staff': 795,\n",
       " 'randi': 796,\n",
       " 'sexi': 797,\n",
       " 'femal': 798,\n",
       " 'local': 799,\n",
       " 'luv': 800,\n",
       " 'netcollex': 801,\n",
       " 'ltd': 802,\n",
       " 'ummma': 803,\n",
       " 'begin': 804,\n",
       " 'qatar': 805,\n",
       " 'pray': 806,\n",
       " 'hard': 807,\n",
       " 'delet': 808,\n",
       " 'sindu': 809,\n",
       " 'birla': 810,\n",
       " 'soft': 811,\n",
       " 'wine': 812,\n",
       " 'flow': 813,\n",
       " 'thk': 814,\n",
       " 'plaza': 815,\n",
       " 'typic': 816,\n",
       " 'everywher': 817,\n",
       " 'dirt': 818,\n",
       " 'floor': 819,\n",
       " 'window': 820,\n",
       " 'shirt': 821,\n",
       " 'sometim': 822,\n",
       " 'mouth': 823,\n",
       " 'dream': 824,\n",
       " 'without': 825,\n",
       " 'chore': 826,\n",
       " 'joy': 827,\n",
       " 'tv': 828,\n",
       " 'exist': 829,\n",
       " 'hail': 830,\n",
       " 'mist': 831,\n",
       " 'becom': 832,\n",
       " 'aaooooright': 833,\n",
       " 'leav': 834,\n",
       " 'hous': 835,\n",
       " 'interview': 836,\n",
       " 'boy': 837,\n",
       " 'annonc': 838,\n",
       " 'arrang': 839,\n",
       " 'keep': 840,\n",
       " 'safe': 841,\n",
       " 'becaus': 842,\n",
       " 'envi': 843,\n",
       " 'everyon': 844,\n",
       " 'parent': 845,\n",
       " 'hand': 846,\n",
       " 'excit': 847,\n",
       " 'each': 848,\n",
       " 'spend': 849,\n",
       " 'bootydeli': 850,\n",
       " 'bangbab': 851,\n",
       " 'order': 852,\n",
       " 'should': 853,\n",
       " 'content': 854,\n",
       " 'goto': 855,\n",
       " 'bangb': 856,\n",
       " 'menu': 857,\n",
       " 'cultur': 858,\n",
       " 'modul': 859,\n",
       " 'avoid': 860,\n",
       " 'missunderstd': 861,\n",
       " 'wit': 862,\n",
       " 'belov': 863,\n",
       " 'escap': 864,\n",
       " 'fanci': 865,\n",
       " 'bridg': 866,\n",
       " 'lager': 867,\n",
       " 'complet': 868,\n",
       " 'form': 869,\n",
       " 'clark': 870,\n",
       " 'also': 871,\n",
       " 'utter': 872,\n",
       " 'wast': 873,\n",
       " 'axi': 874,\n",
       " 'bank': 875,\n",
       " 'hmmm': 876,\n",
       " 'hop': 877,\n",
       " 'muz': 878,\n",
       " 'discuss': 879,\n",
       " 'liao': 880,\n",
       " 'bloodi': 881,\n",
       " 'hell': 882,\n",
       " 'cant': 883,\n",
       " 'believ': 884,\n",
       " 'surnam': 885,\n",
       " 'mr': 886,\n",
       " 'ill': 887,\n",
       " 'clue': 888,\n",
       " 'spanish': 889,\n",
       " 'bath': 890,\n",
       " 'carlo': 891,\n",
       " 'mall': 892,\n",
       " 'stay': 893,\n",
       " 'til': 894,\n",
       " 'smoke': 895,\n",
       " 'worth': 896,\n",
       " 'doesnt': 897,\n",
       " 'log': 898,\n",
       " 'spoke': 899,\n",
       " 'maneesha': 900,\n",
       " 'satisfi': 901,\n",
       " 'experi': 902,\n",
       " 'toll': 903,\n",
       " 'lift': 904,\n",
       " 'especi': 905,\n",
       " 'approach': 906,\n",
       " 'studi': 907,\n",
       " 'trust': 908,\n",
       " 'guy': 909,\n",
       " 'bye': 910,\n",
       " 'handsom': 911,\n",
       " 'toward': 912,\n",
       " 'mummi': 913,\n",
       " 'boytoy': 914,\n",
       " 'awesom': 915,\n",
       " 'minut': 916,\n",
       " 'freephon': 917,\n",
       " 'xma': 918,\n",
       " 'radio': 919,\n",
       " 'ju': 920,\n",
       " 'si': 921,\n",
       " 'uniqu': 922,\n",
       " 'august': 923,\n",
       " 'areyouuniqu': 924,\n",
       " 'leagu': 925,\n",
       " 'touch': 926,\n",
       " 'deal': 927,\n",
       " 'cours': 928,\n",
       " 'itself': 929,\n",
       " 'howev': 930,\n",
       " 'suggest': 931,\n",
       " 'abl': 932,\n",
       " 'everi': 933,\n",
       " 'stool': 934,\n",
       " 'settl': 935,\n",
       " 'wishin': 936,\n",
       " 'mrng': 937,\n",
       " 'hav': 938,\n",
       " 'stori': 939,\n",
       " 'hamster': 940,\n",
       " 'dead': 941,\n",
       " 'tmr': 942,\n",
       " 'orchard': 943,\n",
       " 'mrt': 944,\n",
       " 'kate': 945,\n",
       " 'babyjontet': 946,\n",
       " 'found': 947,\n",
       " 'enc': 948,\n",
       " 'buck': 949,\n",
       " 'darlin': 950,\n",
       " 'ive': 951,\n",
       " 'colleg': 952,\n",
       " 'refil': 953,\n",
       " 'success': 954,\n",
       " 'inr': 955,\n",
       " 'keralacircl': 956,\n",
       " 'prepaid': 957,\n",
       " 'balanc': 958,\n",
       " 'rs': 959,\n",
       " 'transact': 960,\n",
       " 'id': 961,\n",
       " 'kr': 962,\n",
       " 'goodmorn': 963,\n",
       " 'sleep': 964,\n",
       " 'ga': 965,\n",
       " 'alter': 966,\n",
       " 'dat': 967,\n",
       " 'ericsson': 968,\n",
       " 'oso': 969,\n",
       " 'cannot': 970,\n",
       " 'oredi': 971,\n",
       " 'straight': 972,\n",
       " 'dogg': 973,\n",
       " 'connect': 974,\n",
       " 'refund': 975,\n",
       " 'bill': 976,\n",
       " 'both': 977,\n",
       " 'shoot': 978,\n",
       " 'big': 979,\n",
       " 'readi': 980,\n",
       " 'bruv': 981,\n",
       " 'break': 982,\n",
       " 'semest': 983,\n",
       " 'noe': 984,\n",
       " 'leh': 985,\n",
       " 'sound': 986,\n",
       " 'head': 987,\n",
       " 'slept': 988,\n",
       " 'past': 989,\n",
       " 'few': 990,\n",
       " 'easi': 991,\n",
       " 'sen': 992,\n",
       " 'exam': 993,\n",
       " 'march': 994,\n",
       " 'gt': 995,\n",
       " 'atm': 996,\n",
       " 'regist': 997,\n",
       " 'os': 998,\n",
       " 'ubandu': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1                                                 v2\n",
       "0  0.0  Go until jurong point, crazy.. Available only ...\n",
       "1  0.0                      Ok lar... Joking wif u oni...\n",
       "2  1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3  0.0  U dun say so early hor... U c already then say...\n",
       "4  0.0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5572)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "delimiters = re.compile(r'[ ,.\"\\'?!:()]')\n",
    "l = []\n",
    "ct = 0\n",
    "for r,c in df.iterrows():\n",
    "    # print(r)\n",
    "    one_hot = np.zeros(vocab_len)\n",
    "    words = delimiters.split(c['v2'].lower())\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        # print(vocab[word])\n",
    "        if word.isalpha() and word in vocab:\n",
    "            one_hot[vocab[word]] += 1\n",
    "    l.append(one_hot)\n",
    "    ct += 1\n",
    "    \n",
    "    # break\n",
    "df['text'] = l\n",
    "df = df.drop('v2', axis=1)\n",
    "df = df.rename(columns={'v1':'label'})\n",
    "len(l), ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0    0.0  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...\n",
       "1    0.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2    1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...\n",
       "3    0.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4    0.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "5    1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "6    0.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "7    0.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "8    1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9    1.0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5014,), (5014,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[:,1], df[:,0], test_size=0.1)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6059)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = np.ones((2, vocab_len))\n",
    "model.shape\n",
    "# 0 for not_spam, 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(X_train, y_train):\n",
    "    model[int(y)] += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228.,  19.,   1., ...,   2.,   2.,   1.],\n",
       "       [ 31.,   6.,   1., ...,   1.,   1.,   1.],\n",
       "       [259.,  25.,   2., ...,   3.,   3.,   2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = np.vstack((model, np.array(model[0]+model[1])))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08552138, 0.00712678, 0.00037509, ..., 0.00075019, 0.00075019,\n",
       "        0.00037509],\n",
       "       [0.01162791, 0.00225056, 0.00037509, ..., 0.00037509, 0.00037509,\n",
       "        0.00037509],\n",
       "       [0.09714929, 0.00937734, 0.00075019, ..., 0.00112528, 0.00112528,\n",
       "        0.00075019]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model/ max(model[2]) # Normalisation\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_spam = sum(y_train)/ len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theoerm: `P(H|E) = P(E|H) * P(H) / P(E)`\n",
    "\n",
    "- In the context of spam detection, the hypothesis is the statement or prediction about whether a given email is spam or not spam.\n",
    "- In the context of Naive Bayes, two key events are considered:\n",
    "    - A (event): The event that an email is spam.\n",
    "    - A′(complement of event A): The event that an email is not spam (ham).\n",
    "\n",
    "Therefore, `P(E|H) = P(H|E) * P(E) / P(H)`\n",
    "\n",
    "ie, `P_spam_given_sentence = P_sentence_given_spam * P_spam / P_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.50378938970881\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "\n",
    "acc = 0\n",
    "for x,y in zip(X_train, y_train):\n",
    "    non_zero_mask = x != 0\n",
    "    \n",
    "    P_sentence = np.prod((x*model[2])[non_zero_mask]) \n",
    "    \n",
    "    P_sentence_given_spam = np.prod((x*model[1])[non_zero_mask])\n",
    "    P_spam_given_sentence = P_sentence_given_spam * P_spam / P_sentence\n",
    "    \n",
    "    P_sentence_given_not_spam = np.prod((x*model[0])[non_zero_mask])\n",
    "    P_not_spam_given_sentence = P_sentence_given_not_spam * (1 - P_spam) / P_sentence\n",
    "\n",
    "    if float(P_spam_given_sentence > P_not_spam_given_sentence) ==  y:\n",
    "        acc += 1\n",
    "\n",
    "print(acc*100/ len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.39784946236558\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "\n",
    "acc = 0\n",
    "for x,y in zip(X_test, y_test):\n",
    "    non_zero_mask = x != 0\n",
    "    \n",
    "    P_sentence = np.prod((x*model[2])[non_zero_mask]) \n",
    "    \n",
    "    P_sentence_given_spam = np.prod((x*model[1])[non_zero_mask])\n",
    "    P_spam_given_sentence = P_sentence_given_spam * P_spam / P_sentence\n",
    "    \n",
    "    P_sentence_given_not_spam = np.prod((x*model[0])[non_zero_mask])\n",
    "    P_not_spam_given_sentence = P_sentence_given_not_spam * (1 - P_spam) / P_sentence\n",
    "\n",
    "    if float(P_spam_given_sentence > P_not_spam_given_sentence) ==  y:\n",
    "        acc += 1\n",
    "\n",
    "print(acc*100/ len(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
